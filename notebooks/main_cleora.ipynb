{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "path_main_dir = \"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = f\"{path_main_dir}/output/\"\n",
    "\n",
    "config = {\n",
    "    #embedding computation\n",
    "    'cleora_n_iter': 5,\n",
    "    'cleora_dim': 1024,\n",
    "\n",
    "    #dataset preparation\n",
    "    'train_test_split': 0.2,\n",
    "    'dataset_name': 'fb-pages',\n",
    "    #options: fb-pages, walmart-trips, trivago-clicks, trivago-clicks-continents\n",
    "    #modifiers: -resampled-node/-resampled-hyperedge for all datasets\n",
    "\n",
    "    #training classification\n",
    "    'input_embeddings': [\n",
    "        output_dir + 'emb__cluster_id__StarNode.out',\n",
    "        output_dir + 'emb__CliqueNode__CliqueNode.out',\n",
    "    ],\n",
    "    'batch_size': 256,\n",
    "    'test_batch_size': 1000,\n",
    "    'epochs': [20],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read and split data\n",
    "Temporary test and train files are written in output directory."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{path_main_dir}/data/{config['dataset_name']}/node-labels-{config['dataset_name']}.csv\")\n",
    "clique_input_filename = f\"{path_main_dir}/data/{config['dataset_name']}/hyperedges-{config['dataset_name']}.txt\"\n",
    "star_input_filename = f\"{path_main_dir}/data/{config['dataset_name']}/star-{config['dataset_name']}.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_filename = output_dir + \"train_set.txt\"\n",
    "test_filename = output_dir + \"test_set.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df, test_size=config['train_test_split'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(train_filename, \"w\") as f_train:\n",
    "    for index, row in train_set.iterrows():\n",
    "        f_train.write(f\"{row['id']} {row['target']}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(test_filename, \"w\") as f_test:\n",
    "    for index, row in test_set.iterrows():\n",
    "        f_test.write(f\"{row['id']} {row['target']}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Cleora training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Download an appropriate binary Cleora release from: https://github.com/Synerise/cleora/releases . \n",
    "\n",
    "A Linux GNU version is assumed in this example, but any other will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "def columns2output_filename(output_dir, columns):\n",
    "    columns_split = columns.split()\n",
    "    if len(columns_split) == 1 and 'reflexive' in columns:\n",
    "        column_name = columns.split('::')[-1]\n",
    "        return os.path.join(output_dir, f'emb__{column_name}__{column_name}.out')\n",
    "\n",
    "    column_names = [i.split('::')[-1] for i in columns_split]\n",
    "    return os.path.join(output_dir, 'emb__' + '__'.join(column_names) + '.out')\n",
    "\n",
    "\n",
    "def train_cleora(dim, n_iter, columns, input_filename, output_dir):\n",
    "    command = [f'{path_main_dir}/cleora-master/cleora-v1.1.1-x86_64-pc-windows-msvc',\n",
    "               '--columns', columns,\n",
    "               '--dimension', str(dim),\n",
    "               '-n', str(n_iter),\n",
    "               '--input', input_filename,\n",
    "               '-o', output_dir]\n",
    "    subprocess.run(command, check=True)\n",
    "    return columns2output_filename(output_dir, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Star expansion\n",
    "\n",
    "In the `fb_cleora_input_star.txt` file the first column is a virtual node. The parameter `-c \"transient::cluster_id node\"` means that embeddings will not be created for nodes from this column. This translates to star expansion scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "cleora_output_star_filename = train_cleora(config['cleora_dim'],\n",
    "                                           config['cleora_n_iter'],\n",
    "                                           \"transient::cluster_id StarNode\",\n",
    "                                           star_input_filename,\n",
    "                                           output_dir)\n",
    "\n",
    "print(round(time.time() - start_time, 2), \"[s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Clique expansion\n",
    "\n",
    "The `fb_cleora_input_clique.txt` file has the structure of adjacency list. The parameter `-c \"complex::reflexive::node\"` means that edges will be created for all cominations of nodes from each line. This translates to clique expansion scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "cleora_output_clique_filename = train_cleora(config['cleora_dim'],\n",
    "                                             config['cleora_n_iter'],\n",
    "                                             \"complex::reflexive::CliqueNode\",\n",
    "                                             clique_input_filename,\n",
    "                                             output_dir)\n",
    "print(round(time.time() - start_time, 2), \"[s]\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## No expansion\n",
    "\n",
    "You can also compute Cleora without any expansion scheme by providing an input file in the edgelist format (single pair of nodes per line). Run with a simple parameter: `-c \"node1 node2\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Classification\n",
    "\n",
    "We train a simple multiclass Logistic Regression classifier to predict the class of node based on its embedding. We assess the quality of the classifier with of 2 metrics: micro-F1 and macro-F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_embeddings(input_file):\n",
    "    df_full = pd.read_csv(input_file, delimiter=\" \", skiprows=[0], header=None,\n",
    "                          index_col=0)\n",
    "    df_full = df_full.drop([1], axis=1)\n",
    "\n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_train_test(embeddings):\n",
    "    valid_idx = embeddings.index.to_numpy()\n",
    "\n",
    "    train = np.loadtxt(train_filename, delimiter=\" \", dtype=int)\n",
    "    test = np.loadtxt(test_filename, delimiter=\" \", dtype=int)\n",
    "\n",
    "    train = train[np.isin(train[:, 0], valid_idx) & np.isin(train[:, 1], valid_idx)]\n",
    "    test = [t for t in test if (t[0] in valid_idx) and (t[1] in valid_idx)]\n",
    "\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare output file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config_params = config.copy()\n",
    "config_params.pop(\"input_embeddings\")\n",
    "multi_index = list(config_params.keys())\n",
    "multi_index.insert(0, multi_index.pop(multi_index.index('dataset_name')))\n",
    "\n",
    "results_path = output_dir + \"results.csv\"\n",
    "if os.path.exists(results_path):\n",
    "    output_df = pd.read_csv(output_dir + \"results.csv\")\n",
    "    output_df.set_index(multi_index, inplace=True, append=False, drop=False)\n",
    "else:\n",
    "    output_df = pd.DataFrame(index=multi_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = config['batch_size']\n",
    "test_batch_size = config['test_batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = config.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, f1_score\n",
    "\n",
    "targets = df['target'].unique()\n",
    "\n",
    "for algo in config['input_embeddings']:\n",
    "    embeddings = read_embeddings(algo)\n",
    "    train, test = read_train_test(embeddings)\n",
    "\n",
    "    y_train = train[:, 1]\n",
    "    y_true = test[:, 1]\n",
    "\n",
    "    clf = SGDClassifier(random_state=0, loss='log_loss', alpha=0.0001, verbose=0)\n",
    "    for e in tqdm(range(0, max(config['epochs']))):\n",
    "        for idx in range(0, train.shape[0], batch_size):\n",
    "            ex = train[idx:min(idx + batch_size, train.shape[0]), :]\n",
    "\n",
    "            ex_emb_in = embeddings.loc[ex[:, 0]].to_numpy()\n",
    "            ex_y = y_train[idx:min(idx + batch_size, train.shape[0])]\n",
    "\n",
    "            # print(\"Learning loss:\", clf.loss_function_(ex_emb_in,ex_y))\n",
    "            clf.partial_fit(ex_emb_in, ex_y, classes=targets)\n",
    "\n",
    "        if e + 1 in config['epochs']:\n",
    "            acc = 0.0\n",
    "            y_pred = []\n",
    "            y_pred_probas = []\n",
    "            for n, idx in enumerate(range(0, test.shape[0], test_batch_size)):\n",
    "                ex = test[idx:min(idx + test_batch_size, train.shape[0]), :]\n",
    "                ex_emb_in = embeddings.loc[ex[:, 0]].to_numpy()\n",
    "\n",
    "                pred = clf.predict_proba(ex_emb_in)\n",
    "                y_pred_probas.extend(pred)\n",
    "\n",
    "                classes = np.argmax(pred, axis=1)\n",
    "                y_pred.extend(classes)\n",
    "\n",
    "            metrics = {'f1_micro': f1_score(y_true, y_pred, average='micro'),\n",
    "                       'f1_macro': f1_score(y_true, y_pred, average='macro'),\n",
    "                       'log_loss': log_loss(y_true, y_pred_probas)\n",
    "                       }\n",
    "            metrics.update({'algo': algo})\n",
    "            print(metrics)\n",
    "\n",
    "            config_params.update(metrics)\n",
    "            output_df = pd.concat([pd.DataFrame(config_params), output_df]).drop_duplicates()\n",
    "\n",
    "output_df.dropna(inplace=True)\n",
    "output_df.to_csv(output_dir + \"results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}